{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6631385f-41f3-4ac5-b130-3f23bd7b8dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.10.2.post1-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Downloading audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn>=0.20.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (1.6.1)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (1.4.2)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Downloading numba-0.61.0-cp310-cp310-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting soundfile>=0.12.1 (from librosa)\n",
      "  Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl.metadata (16 kB)\n",
      "Collecting pooch>=1.1 (from librosa)\n",
      "  Downloading pooch-1.8.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Downloading soxr-0.5.0.post1-cp310-cp310-win_amd64.whl.metadata (5.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from librosa) (4.12.2)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Downloading lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting msgpack>=1.0 (from librosa)\n",
      "  Downloading msgpack-1.1.0-cp310-cp310-win_amd64.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from lazy-loader>=0.1->librosa) (24.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Downloading llvmlite-0.44.0-cp310-cp310-win_amd64.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from pooch>=1.1->librosa) (3.10.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from pooch>=1.1->librosa) (2.32.3)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.5.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.21)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.1.31)\n",
      "Downloading librosa-0.10.2.post1-py3-none-any.whl (260 kB)\n",
      "Downloading audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading msgpack-1.1.0-cp310-cp310-win_amd64.whl (74 kB)\n",
      "Downloading numba-0.61.0-cp310-cp310-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 0.5/2.8 MB 1.9 MB/s eta 0:00:02\n",
      "   ----------- ---------------------------- 0.8/2.8 MB 1.4 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 1.3/2.8 MB 1.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.8 MB 1.9 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.4/2.8 MB 2.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 2.3 MB/s eta 0:00:00\n",
      "Downloading pooch-1.8.2-py3-none-any.whl (64 kB)\n",
      "Downloading soundfile-0.13.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "   ---------------------------------------- 0.0/1.0 MB ? eta -:--:--\n",
      "   ------------------------------ --------- 0.8/1.0 MB 4.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.0/1.0 MB 4.8 MB/s eta 0:00:00\n",
      "Downloading soxr-0.5.0.post1-cp310-cp310-win_amd64.whl (166 kB)\n",
      "Downloading llvmlite-0.44.0-cp310-cp310-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   - -------------------------------------- 1.0/30.3 MB 5.6 MB/s eta 0:00:06\n",
      "   --- ------------------------------------ 2.4/30.3 MB 5.6 MB/s eta 0:00:06\n",
      "   ---- ----------------------------------- 3.7/30.3 MB 6.1 MB/s eta 0:00:05\n",
      "   ------ --------------------------------- 5.2/30.3 MB 6.5 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 7.1/30.3 MB 6.8 MB/s eta 0:00:04\n",
      "   ----------- ---------------------------- 8.7/30.3 MB 6.9 MB/s eta 0:00:04\n",
      "   ------------ --------------------------- 9.7/30.3 MB 6.9 MB/s eta 0:00:04\n",
      "   -------------- ------------------------- 11.3/30.3 MB 6.6 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 12.8/30.3 MB 6.7 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 14.7/30.3 MB 6.9 MB/s eta 0:00:03\n",
      "   --------------------- ------------------ 16.3/30.3 MB 7.0 MB/s eta 0:00:03\n",
      "   ----------------------- ---------------- 18.1/30.3 MB 7.1 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 19.9/30.3 MB 7.2 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 22.3/30.3 MB 7.4 MB/s eta 0:00:02\n",
      "   -------------------------------- ------- 24.4/30.3 MB 7.6 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 26.0/30.3 MB 7.8 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 28.8/30.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 7.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 7.7 MB/s eta 0:00:00\n",
      "Installing collected packages: soxr, msgpack, llvmlite, lazy-loader, audioread, soundfile, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.1 lazy-loader-0.4 librosa-0.10.2.post1 llvmlite-0.44.0 msgpack-1.1.0 numba-0.61.0 pooch-1.8.2 soundfile-0.13.1 soxr-0.5.0.post1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "30ef728c-c378-404e-b753-a8a3aec3eade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyaudioNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading PyAudio-0.2.14-cp310-cp310-win_amd64.whl.metadata (2.7 kB)\n",
      "Downloading PyAudio-0.2.14-cp310-cp310-win_amd64.whl (164 kB)\n",
      "Installing collected packages: pyaudio\n",
      "Successfully installed pyaudio-0.2.14\n"
     ]
    }
   ],
   "source": [
    "pip install pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f8cb0d2b-c2de-4119-8955-a1e6c23b827c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total .wav files: 0\n",
      "Sample files: []\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = r\"C:\\Users\\akram\\Downloads\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "# List only .wav files\n",
    "wav_files = [f for f in os.listdir(data_path) if f.endswith(\".wav\")]\n",
    "print(f\"Total .wav files: {len(wav_files)}\")\n",
    "print(f\"Sample files: {wav_files[:5]}\")  # Show first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0be74e4e-2ba3-46c3-9e28-a47a5f0cbcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subdirectories: ['Actor_01', 'Actor_02', 'Actor_03', 'Actor_04', 'Actor_05', 'Actor_06', 'Actor_07', 'Actor_08', 'Actor_09', 'Actor_10', 'Actor_11', 'Actor_12', 'Actor_13', 'Actor_14', 'Actor_15', 'Actor_16', 'Actor_17', 'Actor_18', 'Actor_19', 'Actor_20', 'Actor_21', 'Actor_22', 'Actor_23', 'Actor_24']\n",
      "Files in Actor_01: ['03-02-01-01-01-01-01.wav', '03-02-01-01-01-02-01.wav', '03-02-01-01-02-01-01.wav', '03-02-01-01-02-02-01.wav', '03-02-02-01-01-01-01.wav']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "data_path = r\"C:\\Users\\akram\\Downloads\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "# List all subdirectories\n",
    "subdirs = [f for f in os.listdir(data_path) if os.path.isdir(os.path.join(data_path, f))]\n",
    "print(f\"Subdirectories: {subdirs}\")\n",
    "\n",
    "# Check files inside the first subdirectory\n",
    "if subdirs:\n",
    "    first_subdir = os.path.join(data_path, subdirs[0])\n",
    "    files = os.listdir(first_subdir)\n",
    "    print(f\"Files in {subdirs[0]}: {files[:5]}\")  # Show first 5 files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b8becbf-b728-42bb-ad1d-eaf51f42f376",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded: 809 train samples, 203 test samples\n",
      "Classes: ['Angry' 'Calm' 'Fearful' 'Happy' 'Neutral' 'Sad']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import librosa\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Feature extraction function\n",
    "def extract_features(file_path, max_pad_len=174):\n",
    "    try:\n",
    "        audio, sample_rate = librosa.load(file_path, sr=None)  # Load audio\n",
    "        mfccs = librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40)  # Extract MFCCs\n",
    "        pad_width = max_pad_len - mfccs.shape[1]  # Pad or trim to fixed length\n",
    "        if pad_width > 0:\n",
    "            mfccs = np.pad(mfccs, pad_width=((0, 0), (0, pad_width)), mode='constant')\n",
    "        else:\n",
    "            mfccs = mfccs[:, :max_pad_len]\n",
    "        return mfccs\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Dataset path\n",
    "data_path = r\"C:\\Users\\akram\\Downloads\\Audio_Song_Actors_01-24\"\n",
    "\n",
    "# Emotion labels from filename\n",
    "emotions = {\"01\": \"Neutral\", \"02\": \"Calm\", \"03\": \"Happy\", \"04\": \"Sad\", \n",
    "            \"05\": \"Angry\", \"06\": \"Fearful\", \"07\": \"Disgust\", \"08\": \"Surprised\"}\n",
    "\n",
    "features, labels = [], []\n",
    "\n",
    "# Recursively scan all subdirectories\n",
    "for subdir in os.listdir(data_path):\n",
    "    subdir_path = os.path.join(data_path, subdir)\n",
    "    if os.path.isdir(subdir_path):  # Only process folders\n",
    "        for file in os.listdir(subdir_path):\n",
    "            if file.endswith(\".wav\"):\n",
    "                try:\n",
    "                    emotion_label = emotions[file.split(\"-\")[2]]  # Extract emotion from filename\n",
    "                    file_path = os.path.join(subdir_path, file)\n",
    "                    feature = extract_features(file_path)\n",
    "                    if feature is not None:\n",
    "                        features.append(feature)\n",
    "                        labels.append(emotion_label)\n",
    "                except KeyError:\n",
    "                    print(f\"Skipping unknown emotion label in file: {file}\")\n",
    "\n",
    "# Convert lists to NumPy arrays\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(y)  # Convert emotions to numerical labels\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Reshape for LSTM (samples, timesteps, features, 1)\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], X_train.shape[2], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], X_test.shape[2], 1)\n",
    "\n",
    "# Output dataset info\n",
    "print(f\"Dataset loaded: {len(X_train)} train samples, {len(X_test)} test samples\")\n",
    "print(f\"Classes: {label_encoder.classes_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d33df612-4c88-406b-b125-06ece3a0238e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 87ms/step - accuracy: 0.1819 - loss: 94.3556 - val_accuracy: 0.1823 - val_loss: 1.7905\n",
      "Epoch 2/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1801 - loss: 1.7866 - val_accuracy: 0.1823 - val_loss: 1.8130\n",
      "Epoch 3/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.1745 - loss: 1.8134 - val_accuracy: 0.1823 - val_loss: 1.7857\n",
      "Epoch 4/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.1859 - loss: 1.7841 - val_accuracy: 0.1823 - val_loss: 1.7835\n",
      "Epoch 5/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1850 - loss: 1.7834 - val_accuracy: 0.1823 - val_loss: 1.7816\n",
      "Epoch 6/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1949 - loss: 1.7855 - val_accuracy: 0.1823 - val_loss: 1.7798\n",
      "Epoch 7/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1850 - loss: 1.7810 - val_accuracy: 0.1823 - val_loss: 1.7782\n",
      "Epoch 8/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.1838 - loss: 1.7793 - val_accuracy: 0.1823 - val_loss: 1.7768\n",
      "Epoch 9/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.1447 - loss: 1.7795 - val_accuracy: 0.1823 - val_loss: 1.7755\n",
      "Epoch 10/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.2010 - loss: 1.7760 - val_accuracy: 0.1823 - val_loss: 1.7743\n",
      "Epoch 11/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.1498 - loss: 1.7783 - val_accuracy: 0.1823 - val_loss: 1.7734\n",
      "Epoch 12/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.1711 - loss: 1.7767 - val_accuracy: 0.1823 - val_loss: 1.7726\n",
      "Epoch 13/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.1829 - loss: 1.7760 - val_accuracy: 0.1823 - val_loss: 1.7718\n",
      "Epoch 14/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.1687 - loss: 1.7721 - val_accuracy: 0.1823 - val_loss: 1.7712\n",
      "Epoch 15/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.1733 - loss: 1.7783 - val_accuracy: 0.1823 - val_loss: 1.7707\n",
      "Epoch 16/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1916 - loss: 1.7692 - val_accuracy: 0.1823 - val_loss: 1.7701\n",
      "Epoch 17/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.1645 - loss: 1.7648 - val_accuracy: 0.1823 - val_loss: 1.7697\n",
      "Epoch 18/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1830 - loss: 1.7685 - val_accuracy: 0.1823 - val_loss: 1.7694\n",
      "Epoch 19/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.1737 - loss: 1.7687 - val_accuracy: 0.1823 - val_loss: 1.7691\n",
      "Epoch 20/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 82ms/step - accuracy: 0.1359 - loss: 1.7754 - val_accuracy: 0.1823 - val_loss: 1.7687\n",
      "Epoch 21/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.2052 - loss: 1.7687 - val_accuracy: 0.1823 - val_loss: 1.7684\n",
      "Epoch 22/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 80ms/step - accuracy: 0.1738 - loss: 1.7626 - val_accuracy: 0.1823 - val_loss: 1.7681\n",
      "Epoch 23/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1581 - loss: 1.7711 - val_accuracy: 0.1823 - val_loss: 1.7680\n",
      "Epoch 24/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.1639 - loss: 1.7735 - val_accuracy: 0.1823 - val_loss: 1.7678\n",
      "Epoch 25/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 87ms/step - accuracy: 0.1874 - loss: 1.7731 - val_accuracy: 0.1823 - val_loss: 1.7676\n",
      "Epoch 26/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1849 - loss: 1.7663 - val_accuracy: 0.1823 - val_loss: 1.7674\n",
      "Epoch 27/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 84ms/step - accuracy: 0.1510 - loss: 1.7649 - val_accuracy: 0.1823 - val_loss: 1.7673\n",
      "Epoch 28/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1664 - loss: 2.2608 - val_accuracy: 0.1823 - val_loss: 1.7672\n",
      "Epoch 29/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 75ms/step - accuracy: 0.1951 - loss: 1.7748 - val_accuracy: 0.1823 - val_loss: 1.7671\n",
      "Epoch 30/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1824 - loss: 1.7833 - val_accuracy: 0.1823 - val_loss: 1.7670\n",
      "Epoch 31/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.1849 - loss: 1.7631 - val_accuracy: 0.1823 - val_loss: 1.7669\n",
      "Epoch 32/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 85ms/step - accuracy: 0.1854 - loss: 1.7592 - val_accuracy: 0.1823 - val_loss: 1.7669\n",
      "Epoch 33/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1809 - loss: 1.7706 - val_accuracy: 0.1823 - val_loss: 1.7668\n",
      "Epoch 34/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 73ms/step - accuracy: 0.1810 - loss: 1.7649 - val_accuracy: 0.1823 - val_loss: 1.7668\n",
      "Epoch 35/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 69ms/step - accuracy: 0.1672 - loss: 1.7683 - val_accuracy: 0.1823 - val_loss: 1.7667\n",
      "Epoch 36/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1924 - loss: 1.7671 - val_accuracy: 0.1823 - val_loss: 1.7667\n",
      "Epoch 37/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1516 - loss: 1.7729 - val_accuracy: 0.1823 - val_loss: 1.7666\n",
      "Epoch 38/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1727 - loss: 1.7663 - val_accuracy: 0.1823 - val_loss: 1.7666\n",
      "Epoch 39/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.1694 - loss: 1.7708 - val_accuracy: 0.1823 - val_loss: 1.7666\n",
      "Epoch 40/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1613 - loss: 1.7667 - val_accuracy: 0.1823 - val_loss: 1.7665\n",
      "Epoch 41/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 81ms/step - accuracy: 0.1758 - loss: 1.7722 - val_accuracy: 0.1823 - val_loss: 1.7665\n",
      "Epoch 42/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1764 - loss: 1.7666 - val_accuracy: 0.1823 - val_loss: 1.7665\n",
      "Epoch 43/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.1776 - loss: 1.7657 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 44/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 78ms/step - accuracy: 0.1927 - loss: 1.7743 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 45/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1390 - loss: 1.7685 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 46/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1781 - loss: 1.7665 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 47/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 77ms/step - accuracy: 0.1868 - loss: 1.7790 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 48/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 83ms/step - accuracy: 0.1400 - loss: 1.7637 - val_accuracy: 0.1823 - val_loss: 1.7664\n",
      "Epoch 49/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 76ms/step - accuracy: 0.1623 - loss: 1.7655 - val_accuracy: 0.1823 - val_loss: 1.7663\n",
      "Epoch 50/50\n",
      "\u001b[1m51/51\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 79ms/step - accuracy: 0.2026 - loss: 1.7568 - val_accuracy: 0.1823 - val_loss: 1.7663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model training complete and saved!\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "\n",
    "# Build model\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(40, 174, 1)),  # CNN Layer\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    Flatten(),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y)), activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "# Compile model\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train model\n",
    "model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test))\n",
    "\n",
    "# Save model\n",
    "model.save(\"voice_emotion_model.h5\")\n",
    "print(\"Model training complete and saved!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a7596fa6-7d9b-4b75-a66d-ef82ae6a7b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording complete!\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 74ms/step\n",
      "Predicted Emotion: Calm\n"
     ]
    }
   ],
   "source": [
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(output_file, duration=3, sample_rate=22050):\n",
    "    \"\"\"Record audio for a given duration and save it as a WAV file.\"\"\"\n",
    "    chunk = 1024\n",
    "    format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=format, channels=channels, rate=sample_rate, input=True, frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(sample_rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording complete!\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    wf = wave.open(output_file, \"wb\")\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(audio.get_sample_size(format))\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n",
    "\n",
    "def predict_emotion(file_path):\n",
    "    \"\"\"Predict emotion from an audio file.\"\"\"\n",
    "    model = tf.keras.models.load_model(\"voice_emotion_model.h5\")\n",
    "    feature = extract_features(file_path)\n",
    "    if feature is not None:\n",
    "        feature = feature.reshape(1, feature.shape[0], feature.shape[1], 1)\n",
    "        prediction = model.predict(feature)\n",
    "        emotion = label_encoder.inverse_transform([np.argmax(prediction)])[0]\n",
    "        print(f\"Predicted Emotion: {emotion}\")\n",
    "    else:\n",
    "        print(\"Error in extracting features!\")\n",
    "\n",
    "# Record and predict emotion\n",
    "record_audio(\"test_audio.wav\", duration=3)\n",
    "predict_emotion(\"test_audio.wav\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dd172538-cea2-4cca-87f0-8dbf5507eac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\akram\\anaconda3\\envs\\mediapipe_env\\lib\\site-packages\\keras\\src\\layers\\convolutional\\base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, LSTM, Dense, Dropout, TimeDistributed\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation=\"relu\", input_shape=(40, 174, 1)),  # CNN feature extraction\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation=\"relu\"),\n",
    "    MaxPooling2D(pool_size=(2, 2)),\n",
    "    Dropout(0.2),\n",
    "    \n",
    "    TimeDistributed(Flatten()),  # Converts CNN features for LSTM\n",
    "    LSTM(64, return_sequences=True),  \n",
    "    LSTM(64),  \n",
    "\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dropout(0.3),\n",
    "    Dense(len(np.unique(y)), activation=\"softmax\")  # Output layer\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b956e775-3841-403e-b67b-94c211362d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pyaudio\n",
    "import wave\n",
    "\n",
    "def record_audio(output_file, duration=3, sample_rate=44100):\n",
    "    \"\"\"Record audio for a given duration and save it as a WAV file.\"\"\"\n",
    "    chunk = 1024\n",
    "    format = pyaudio.paInt16\n",
    "    channels = 1\n",
    "\n",
    "    audio = pyaudio.PyAudio()\n",
    "    stream = audio.open(format=format, channels=channels, rate=sample_rate, input=True, frames_per_buffer=chunk)\n",
    "\n",
    "    print(\"Starting in 2 seconds... Speak clearly.\")\n",
    "    time.sleep(2)\n",
    "\n",
    "    print(\"Recording...\")\n",
    "    frames = []\n",
    "    for _ in range(0, int(sample_rate / chunk * duration)):\n",
    "        data = stream.read(chunk)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"Recording complete!\")\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    audio.terminate()\n",
    "\n",
    "    wf = wave.open(output_file, \"wb\")\n",
    "    wf.setnchannels(channels)\n",
    "    wf.setsampwidth(audio.get_sample_size(format))\n",
    "    wf.setframerate(sample_rate)\n",
    "    wf.writeframes(b\"\".join(frames))\n",
    "    wf.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f130b2-e05e-44f3-bba6-f7b223d4f5e9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
